{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lesson4-imdb.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["l6XRE8Lq_0Rv","PeZ23sbT_0SU","Xp444gb8_0Se","_Zi-z4nU_0TL"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"lnWVRz9A_33j","colab_type":"text"},"cell_type":"markdown","source":["# Colab Setup"]},{"metadata":{"id":"-FNMHY8k_2sq","colab_type":"code","outputId":"ba5f023a-9074-4d69-b6bb-8d40085d7e74","executionInfo":{"status":"ok","timestamp":1545335866984,"user_tz":-60,"elapsed":167600,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":3589}},"cell_type":"code","source":["!pip install Pillow==4.1.1\n","!pip install --force-reinstall scipy\n","\n","!pip install prompt-toolkit==1.0.15\n","\n","\n","!pip install fastai==0.7.0 \n","!pip install torchtext==0.2.3\n","\n","#!pip install --force-reinstall \"pandas==0.22\"\n","#!pip install --force-reinstall \"pandas_summary>=0.0.41\"\n","\n","!pip install kaggle --upgrade\n","\n","!pip install -U spacy\n","!python3 -m spacy download en\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting Pillow==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n","\u001b[K    100% |████████████████████████████████| 5.7MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.1.1) (0.46)\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed Pillow-4.1.1\n","Collecting scipy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n","\u001b[K    100% |████████████████████████████████| 26.6MB 1.5MB/s \n","\u001b[?25hCollecting numpy>=1.8.2 (from scipy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n","\u001b[K    100% |████████████████████████████████| 13.9MB 4.1MB/s \n","\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n","\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy\n","  Found existing installation: numpy 1.14.6\n","    Uninstalling numpy-1.14.6:\n","      Successfully uninstalled numpy-1.14.6\n","  Found existing installation: scipy 1.1.0\n","    Uninstalling scipy-1.1.0:\n","      Successfully uninstalled scipy-1.1.0\n","Successfully installed numpy-1.15.4 scipy-1.2.0\n","Requirement already satisfied: prompt-toolkit==1.0.15 in /usr/local/lib/python3.6/dist-packages (1.0.15)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.15) (1.11.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.15) (0.1.7)\n","Collecting fastai==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/6d/9d0d6e17a78b0598d5e8c49a0d03ffc7ff265ae62eca3e2345fab14edb9b/fastai-0.7.0-py3-none-any.whl (112kB)\n","\u001b[K    100% |████████████████████████████████| 122kB 3.4MB/s \n","\u001b[?25hCollecting pandas-summary (from fastai==0.7.0)\n","  Downloading https://files.pythonhosted.org/packages/97/55/ea54109a4e7a8e7342bdf23e9382c858224263d984b0d95610568e564f59/pandas_summary-0.0.5-py2.py3-none-any.whl\n","Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.5.3)\n","Collecting feather-format (from fastai==0.7.0)\n","  Downloading https://files.pythonhosted.org/packages/08/55/940b97cc6f19a19f5dab9efef2f68a0ce43a7632f858b272391f0b851a7e/feather-format-0.4.0.tar.gz\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (17.0.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.10.1)\n","Collecting jedi (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/bc/54d53f5bc4658380d0eca9055d72be4df45e5bfd91a4bac97da224a92553/jedi-0.13.2-py2.py3-none-any.whl (177kB)\n","\u001b[K    100% |████████████████████████████████| 184kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: ptyprocess in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.6.0)\n","Requirement already satisfied: simplegeneric in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.8.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.1.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.10.0)\n","Collecting bcolz (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n","\u001b[K    100% |████████████████████████████████| 1.5MB 13.5MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.5.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.1.1)\n","Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.1.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.7.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.6.0)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.10)\n","Collecting sklearn-pandas (from fastai==0.7.0)\n","  Downloading https://files.pythonhosted.org/packages/1f/48/4e1461d828baf41d609efaa720d20090ac6ec346b5daad3c88e243e2207e/sklearn_pandas-1.8.0-py2.py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.15.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.28.1)\n","Collecting isoweek (from fastai==0.7.0)\n","  Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.5.1)\n","Requirement already satisfied: widgetsnbextension in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.4.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2018.7)\n","Collecting torchtext (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/bc/b28b9efb4653c03e597ed207264eea45862b5260f48e9f010b5068d64db1/torchtext-0.3.1-py3-none-any.whl (62kB)\n","\u001b[K    100% |████████████████████████████████| 71kB 22.1MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (5.5.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.2.0)\n","Collecting torchvision (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 11.0MB/s \n","\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.6.1)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.2.0)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.3.2)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.4.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2018.11.29)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.7.5)\n","Collecting plotnine (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/02/b171c828560aea3a5da1efda464230dac3ef4f4834b88e0bd52ad14a08f0/plotnine-0.5.1-py2.py3-none-any.whl (3.6MB)\n","\u001b[K    100% |████████████████████████████████| 3.6MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.1.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.0.2)\n","Collecting torch<0.4 (from fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a5/e8b50b55b1abac9f1e3346c4242f1e42a82d368a8442cbd50c532922f6c4/torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n","\u001b[K    100% |████████████████████████████████| 496.4MB 37kB/s \n","\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (1.0.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (4.3.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.4.4.19)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.3.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (3.13)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (0.22.0)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (7.4.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==0.7.0) (2.1.2)\n","Collecting pyarrow>=0.4.0 (from feather-format->fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/94/23135312f97b20d6457294606fb70fad43ef93b7bffe567088ebe3623703/pyarrow-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\n","\u001b[K    100% |████████████████████████████████| 11.6MB 3.0MB/s \n","\u001b[?25hCollecting parso>=0.3.0 (from jedi->fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/51/9c48a46334be50c13d25a3afe55fa05c445699304c5ad32619de953a2305/parso-0.3.1-py2.py3-none-any.whl (88kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 27.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->fastai==0.7.0) (1.11.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai==0.7.0) (0.46)\n","Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas->fastai==0.7.0) (0.20.1)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension->fastai==0.7.0) (5.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->fastai==0.7.0) (2.18.4)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (4.6.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (1.0.15)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai==0.7.0) (40.6.3)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->fastai==0.7.0) (5.2.4)\n","Collecting mizani>=0.5.2 (from plotnine->fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/3a/1d1c5563b6aeb5fffda694b70d649a0f728a112b79a66b85a6af4814a643/mizani-0.5.2-py2.py3-none-any.whl (58kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 24.2MB/s \n","\u001b[?25hRequirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.8.0)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai==0.7.0) (0.5.1)\n","Collecting descartes>=1.1.0 (from plotnine->fastai==0.7.0)\n","  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (5.4.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (4.4.3)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai==0.7.0) (6.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->fastai==0.7.0) (4.4.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.0) (4.4.0)\n","Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension->fastai==0.7.0) (0.8.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (3.0.4)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai==0.7.0) (2.6)\n","Collecting palettable (from mizani>=0.5.2->plotnine->fastai==0.7.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/8a/84537c0354f0d1f03bf644b71bf8e0a50db9c1294181905721a5f3efbf66/palettable-3.1.1-py2.py3-none-any.whl (77kB)\n","\u001b[K    100% |████████████████████████████████| 81kB 27.5MB/s \n","\u001b[?25hRequirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (0.5.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (1.4.2)\n","Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai==0.7.0) (0.8.4)\n","Building wheels for collected packages: feather-format, bcolz\n","  Running setup.py bdist_wheel for feather-format ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/85/7d/12/2dfa5c0195f921ac935f5e8f27deada74972edc0ae9988a9c1\n","  Running setup.py bdist_wheel for bcolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n","Successfully built feather-format bcolz\n","\u001b[31mmizani 0.5.2 has requirement pandas>=0.23.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n","\u001b[31mplotnine 0.5.1 has requirement matplotlib>=3.0.0, but you'll have matplotlib 2.1.2 which is incompatible.\u001b[0m\n","\u001b[31mplotnine 0.5.1 has requirement pandas>=0.23.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n","Installing collected packages: pandas-summary, pyarrow, feather-format, parso, jedi, bcolz, sklearn-pandas, isoweek, torch, torchtext, torchvision, palettable, mizani, descartes, plotnine, fastai\n","Successfully installed bcolz-1.2.1 descartes-1.1.0 fastai-0.7.0 feather-format-0.4.0 isoweek-1.3.3 jedi-0.13.2 mizani-0.5.2 palettable-3.1.1 pandas-summary-0.0.5 parso-0.3.1 plotnine-0.5.1 pyarrow-0.11.1 sklearn-pandas-1.8.0 torch-0.3.1 torchtext-0.3.1 torchvision-0.2.1\n","Collecting torchtext==0.2.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 1.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (2.18.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2018.11.29)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.3) (3.0.4)\n","Building wheels for collected packages: torchtext\n","  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n","Successfully built torchtext\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.2.3\n","Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.1.1)\n","Requirement already satisfied, skipping upgrade: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n","Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n","Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n","Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n","Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.2.6)\n","Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied, skipping upgrade: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n","Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n","Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n","Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.15.4)\n","Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n","Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n","Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n","Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n","Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n","Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n","Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n","Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n","Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n","Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n","Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n","Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n","\n","    You can now load the model via spacy.load('en')\n","\n"],"name":"stdout"}]},{"metadata":{"id":"bhtkNRG9HAQf","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"sV6pLEll__au","colab_type":"text"},"cell_type":"markdown","source":["# Fastai "]},{"metadata":{"id":"IFU3YhwY_0Q0","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","from fastai.learner import *\n","\n","import torchtext\n","from torchtext import vocab, data\n","from torchtext.datasets import language_modeling\n","\n","from fastai.rnn_reg import *\n","from fastai.rnn_train import *\n","from fastai.nlp import *\n","from fastai.lm_rnn import *\n","\n","import dill as pickle\n","import spacy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hpGdeSAB_0Q2","colab_type":"text"},"cell_type":"markdown","source":["# Language modeling"]},{"metadata":{"id":"RE6fvZNBAHKl","colab_type":"code","outputId":"b26b88f3-4bff-4302-b5db-16c4ec85866a","executionInfo":{"status":"ok","timestamp":1545335891268,"user_tz":-60,"elapsed":191860,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"cell_type":"code","source":["!mkdir -p data/ ; cd data/; wget http://files.fast.ai/data/aclImdb.tgz; tar -xzf aclImdb.tgz"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2018-12-20 19:58:02--  http://files.fast.ai/data/aclImdb.tgz\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 145982645 (139M) [text/plain]\n","Saving to: ‘aclImdb.tgz’\n","\n","aclImdb.tgz         100%[===================>] 139.22M  34.8MB/s    in 4.6s    \n","\n","2018-12-20 19:58:07 (30.4 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"zACXKlJUPyiO","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p data/aclImdb/models/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pdFBcG88_0Q4","colab_type":"text"},"cell_type":"markdown","source":["### Data"]},{"metadata":{"id":"I3qO0MbY_0Q4","colab_type":"text"},"cell_type":"markdown","source":["The [large movie view dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n","\n","The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n","\n","However, before we try to classify *sentiment*, we will simply try to create a *language model*; that is, a model that can predict the next word in a sentence. Why? Because our model first needs to understand the structure of English, before we can expect it to recognize positive vs negative sentiment.\n","\n","So our plan of attack is the same as we used for Dogs v Cats: pretrain a model to do one thing (predict the next word), and fine tune it to do something else (classify sentiment).\n","\n","Unfortunately, there are no good pretrained language models available to download, so we need to create our own. To follow along with this notebook, we suggest downloading the dataset from [this location](http://files.fast.ai/data/aclImdb.tgz) on files.fast.ai."]},{"metadata":{"id":"Jhktcc2x_0Q5","colab_type":"code","outputId":"be887baa-c641-43d5-a30e-c6b14569e0d7","executionInfo":{"status":"ok","timestamp":1545335896536,"user_tz":-60,"elapsed":197112,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["PATH='data/aclImdb/'\n","\n","TRN_PATH = 'train/all/'\n","VAL_PATH = 'test/all/'\n","TRN = f'{PATH}{TRN_PATH}'\n","VAL = f'{PATH}{VAL_PATH}'\n","\n","%ls {PATH}"],"execution_count":5,"outputs":[{"output_type":"stream","text":["imdbEr.txt  imdb.vocab  \u001b[0m\u001b[01;34mmodels\u001b[0m/  README  \u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"],"name":"stdout"}]},{"metadata":{"id":"vNUd05o2_0Q9","colab_type":"text"},"cell_type":"markdown","source":["Let's look inside the training folder..."]},{"metadata":{"id":"uwn7PAZu_0Q9","colab_type":"code","outputId":"c6734504-9f8b-463c-b004-f03d3586e8c6","executionInfo":{"status":"ok","timestamp":1545335900780,"user_tz":-60,"elapsed":201342,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"cell_type":"code","source":["trn_files = !ls {TRN}\n","trn_files[:10]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['0_0.txt       1562_10.txt  24997_0.txt\\t34371_0.txt  43748_0.txt  6248_7.txt',\n"," '0_3.txt       15621_0.txt  24998_0.txt\\t3437_1.txt   43749_0.txt  6249_0.txt',\n"," '0_9.txt       1562_1.txt   24999_0.txt\\t34372_0.txt  437_4.txt\\t  6249_2.txt',\n"," '10000_0.txt   15622_0.txt  25000_0.txt\\t34373_0.txt  43750_0.txt  6249_7.txt',\n"," '10000_4.txt   15623_0.txt  2500_0.txt\\t34374_0.txt  4375_0.txt   624_9.txt',\n"," '10000_8.txt   15624_0.txt  25001_0.txt\\t34375_0.txt  43751_0.txt  6250_0.txt',\n"," '1000_0.txt    15625_0.txt  2500_1.txt\\t34376_0.txt  4375_1.txt   6250_10.txt',\n"," '10001_0.txt   15626_0.txt  25002_0.txt\\t34377_0.txt  43752_0.txt  6250_1.txt',\n"," '10001_10.txt  15627_0.txt  25003_0.txt\\t34378_0.txt  43753_0.txt  625_0.txt',\n"," '10001_4.txt   15628_0.txt  25004_0.txt\\t3437_8.txt   43754_0.txt  625_10.txt']"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"65Ds7fmT_0RA","colab_type":"text"},"cell_type":"markdown","source":["...and at an example review."]},{"metadata":{"id":"qW1pBwwa_0RB","colab_type":"code","outputId":"b445b680-b775-4c46-af11-edc9407dc769","executionInfo":{"status":"ok","timestamp":1545335903750,"user_tz":-60,"elapsed":204300,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["review = !cat {TRN}{trn_files[6]}\n","review[0]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop-socky fung-ku, but what I got instead was a comedy. So, it wasn't quite was I was expecting, but I really liked it anyway! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them!! I was laughing my ass off. I mean, the cops were just so bad! And when I say bad, I mean The Shield Vic Macky bad. But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose...man, oh man. What can you say about that hottie. She was great and put those other actresses to shame. She should work more often!!!!! I also really liked the fight scene outside of the building. That was done really well. Lots of fighting and people getting their heads banged up. FUN! Last, but not least Joe Estevez and William Smith were great as the...well, I wasn't sure what they were, but they seemed to be having fun and throwing out lines. I mean, some of it didn't make sense with the rest of the flick, but who cares when you're laughing so hard! All in all the film wasn't the greatest thing since sliced bread, but I wasn't expecting that. It was a Troma flick so I figured it would totally suck. It's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies. Don't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat: 15625_0.txt: No such file or directory\""]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"vELzQS_b_0RE","colab_type":"text"},"cell_type":"markdown","source":["Sounds like I'd really enjoy *Zombiegeddon*...\n","\n","Now we'll check how many words are in the dataset."]},{"metadata":{"id":"gsQEezTp_0RE","colab_type":"code","outputId":"7a5a7d6c-846c-4c31-8409-31a90f4cc4f3","executionInfo":{"status":"ok","timestamp":1545335907585,"user_tz":-60,"elapsed":208127,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["!find {TRN} -name '*.txt' | xargs cat | wc -w"],"execution_count":8,"outputs":[{"output_type":"stream","text":["17486581\n"],"name":"stdout"}]},{"metadata":{"id":"PagFFka__0RH","colab_type":"code","outputId":"2a8f73d2-08af-472c-e981-65f1e539ff3d","executionInfo":{"status":"ok","timestamp":1545335910163,"user_tz":-60,"elapsed":210693,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["!find {VAL} -name '*.txt' | xargs cat | wc -w"],"execution_count":9,"outputs":[{"output_type":"stream","text":["5686719\n"],"name":"stdout"}]},{"metadata":{"id":"U2S1ysIU_0RK","colab_type":"text"},"cell_type":"markdown","source":["Before we can analyze text, we must first *tokenize* it. This refers to the process of splitting a sentence into an array of words (or more generally, into an array of *tokens*)."]},{"metadata":{"id":"94ofuIJz_0RL","colab_type":"code","colab":{}},"cell_type":"code","source":["spacy_tok = spacy.load('en')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A5TKMXBX_0RO","colab_type":"code","outputId":"0c74daa9-45fd-4f18-91b4-5212aa53eee3","executionInfo":{"status":"ok","timestamp":1545335923649,"user_tz":-60,"elapsed":224164,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["' '.join([sent.string.strip() for sent in spacy_tok(review[0])])"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop - socky fung - ku , but what I got instead was a comedy . So , it was n't quite was I was expecting , but I really liked it anyway ! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them ! ! I was laughing my ass off . I mean , the cops were just so bad ! And when I say bad , I mean The Shield Vic Macky bad . But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose ... man , oh man . What can you say about that hottie . She was great and put those other actresses to shame . She should work more often ! ! ! ! ! I also really liked the fight scene outside of the building . That was done really well . Lots of fighting and people getting their heads banged up . FUN ! Last , but not least Joe Estevez and William Smith were great as the ... well , I was n't sure what they were , but they seemed to be having fun and throwing out lines . I mean , some of it did n't make sense with the rest of the flick , but who cares when you 're laughing so hard ! All in all the film was n't the greatest thing since sliced bread , but I was n't expecting that . It was a Troma flick so I figured it would totally suck . It 's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies . Do n't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat : 15625_0.txt : No such file or directory\""]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"GgsVmBMu_0RP","colab_type":"text"},"cell_type":"markdown","source":["We use Pytorch's [torchtext](https://github.com/pytorch/text) library to preprocess our data, telling it to use the wonderful [spacy](https://spacy.io/) library to handle tokenization.\n","\n","First, we create a torchtext *field*, which describes how to preprocess a piece of text - in this case, we tell torchtext to make everything lowercase, and tokenize it with spacy."]},{"metadata":{"id":"Plak5zHR_0RR","colab_type":"code","colab":{}},"cell_type":"code","source":["TEXT = data.Field(lower=True, tokenize=\"spacy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qHM_EI9X_0RU","colab_type":"text"},"cell_type":"markdown","source":["fastai works closely with torchtext. We create a ModelData object for language modeling by taking advantage of `LanguageModelData`, passing it our torchtext field object, and the paths to our training, test, and validation sets. In this case, we don't have a separate test set, so we'll just use `VAL_PATH` for that too.\n","\n","As well as the usual `bs` (batch size) parameter, we also not have `bptt`; this define how many words are processing at a time in each row of the mini-batch. More importantly, it defines how many 'layers' we will backprop through. Making this number higher will increase time and memory requirements, but will improve the model's ability to handle long sentences."]},{"metadata":{"id":"M_o1Vb6g_0RV","colab_type":"code","colab":{}},"cell_type":"code","source":["bs=64; bptt=70"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D9sgzuiH_0RW","colab_type":"code","colab":{}},"cell_type":"code","source":["FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n","md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NEaX1R4j_0RY","colab_type":"text"},"cell_type":"markdown","source":["After building our `ModelData` object, it automatically fills the `TEXT` object with a very important attribute: `TEXT.vocab`. This is a *vocabulary*, which stores which words (or *tokens*) have been seen in the text, and how each word will be mapped to a unique integer id. We'll need to use this information again later, so we save it.\n","\n","*(Technical note: python's standard `Pickle` library can't handle this correctly, so at the top of this notebook we used the `dill` library instead and imported it as `pickle`)*."]},{"metadata":{"id":"aoGp6wwy_0RZ","colab_type":"code","colab":{}},"cell_type":"code","source":["pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yfA1aUUp_0Ra","colab_type":"text"},"cell_type":"markdown","source":["Here are the: # batches; # unique tokens in the vocab; # tokens in the training set; # sentences"]},{"metadata":{"id":"ZemGVhvk_0Rb","colab_type":"code","outputId":"141a27b8-c831-40bc-c90e-64e3ed6eed0a","executionInfo":{"status":"ok","timestamp":1545336231452,"user_tz":-60,"elapsed":531946,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4583, 37392, 1, 20540756)"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"Kufzua-e_0Re","colab_type":"text"},"cell_type":"markdown","source":["This is the start of the mapping from integer IDs to unique tokens."]},{"metadata":{"id":"KHlxUOn6_0Rf","colab_type":"code","outputId":"67cfb05c-5df5-4186-8981-7de345ccb8ab","executionInfo":{"status":"ok","timestamp":1545336231455,"user_tz":-60,"elapsed":531938,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# 'itos': 'int-to-string'\n","TEXT.vocab.itos[:12]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"7AZbgHjcpv6x","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"CmNmlvZK_0Rj","colab_type":"code","outputId":"8b24606e-2c47-43ef-dcfd-7d42dd6f146e","executionInfo":{"status":"ok","timestamp":1545336231457,"user_tz":-60,"elapsed":531929,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# 'stoi': 'string to int'\n","TEXT.vocab.stoi['the']"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"GypYEmK9_0Rl","colab_type":"text"},"cell_type":"markdown","source":["Note that in a `LanguageModelData` object there is only one item in each dataset: all the words of the text joined together."]},{"metadata":{"id":"aWb24ISP_0Rn","colab_type":"code","outputId":"422f93b0-a1a3-4937-bbf7-ee630230e775","executionInfo":{"status":"ok","timestamp":1545336231462,"user_tz":-60,"elapsed":531926,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"cell_type":"code","source":["md.trn_ds[0].text[:12]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'obscure',\n"," 'universal',\n"," '\"',\n"," 'b',\n"," '\"',\n"," 'horror',\n"," 'flick',\n"," 'is',\n"," 'also',\n"," 'included',\n"," 'in']"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"3rF_rnz2_0Rp","colab_type":"text"},"cell_type":"markdown","source":["torchtext will handle turning this words into integer IDs for us automatically."]},{"metadata":{"id":"eqbh7Z-f_0Rp","colab_type":"code","outputId":"9eebbf35-7488-4005-c733-dbf1532a2fe5","executionInfo":{"status":"ok","timestamp":1545336231464,"user_tz":-60,"elapsed":531916,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":263}},"cell_type":"code","source":["TEXT.numericalize([md.trn_ds[0].text[:12]])"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n","   13\n"," 3240\n"," 2273\n","   15\n","  638\n","   15\n","  201\n","  513\n","    9\n","  100\n"," 1967\n","   10\n","[torch.cuda.LongTensor of size 12x1 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"-3MoEjEX_0Rs","colab_type":"text"},"cell_type":"markdown","source":["Our `LanguageModelData` object will create batches with 64 columns (that's our batch size), and varying sequence lengths of around 80 tokens (that's our `bptt` parameter - *backprop through time*).\n","\n","Each batch also contains the exact same data as labels, but one word later in the text - since we're trying to always predict the next word. The labels are flattened into a 1d array."]},{"metadata":{"id":"81IWQ4gj_0Rs","colab_type":"code","outputId":"d0a6d260-cbb2-4375-ceb2-1db88d7fc62c","executionInfo":{"status":"ok","timestamp":1545336231466,"user_tz":-60,"elapsed":531909,"user":{"displayName":"Cesare Montresor","photoUrl":"https://lh4.googleusercontent.com/-lvd8fJRlvXc/AAAAAAAAAAI/AAAAAAAAAPM/AHfrop3EmSY/s64/photo.jpg","userId":"02780399299495258362"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"cell_type":"code","source":["next(iter(md.trn_dl))"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Variable containing:\n","     13     17     40  ...       2     37     19\n","   3240   1238      2  ...     357      9     44\n","   2273     25    596  ...      47     14    226\n","         ...            ⋱           ...         \n","    748     15     10  ...      62     25      6\n","   1099   2583      2  ...     397      9    571\n","  14637    162    408  ...       4      6     11\n"," [torch.cuda.LongTensor of size 70x64 (GPU 0)], Variable containing:\n","   3240\n","   1238\n","      2\n","   ⋮   \n","     48\n","   2169\n","     97\n"," [torch.cuda.LongTensor of size 4480 (GPU 0)])"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"l6XRE8Lq_0Rv","colab_type":"text"},"cell_type":"markdown","source":["### Train"]},{"metadata":{"id":"zzJF9kRo_0Rw","colab_type":"text"},"cell_type":"markdown","source":["We have a number of parameters to set - we'll learn more about these later, but you should find these values suitable for many problems."]},{"metadata":{"id":"HFhU6TuC_0Rw","colab_type":"code","colab":{}},"cell_type":"code","source":["em_sz = 200  # size of each embedding vector\n","nh = 500     # number of hidden activations per layer\n","nl = 3       # number of layers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tjwyM6fm_0Rz","colab_type":"text"},"cell_type":"markdown","source":["Researchers have found that large amounts of *momentum* (which we'll learn about later) don't work well with these kinds of *RNN* models, so we create a version of the *Adam* optimizer with less momentum than it's default of `0.9`."]},{"metadata":{"id":"XjI89jI__0R0","colab_type":"code","colab":{}},"cell_type":"code","source":["opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ux2_hJVU_0R4","colab_type":"text"},"cell_type":"markdown","source":["fastai uses a variant of the state of the art [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182) developed by Stephen Merity. A key feature of this model is that it provides excellent regularization through [Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout). There is no simple way known (yet!) to find the best values of the dropout parameters below - you just have to experiment...\n","\n","However, the other parameters (`alpha`, `beta`, and `clip`) shouldn't generally need tuning."]},{"metadata":{"id":"fGpjyv78_0R5","colab_type":"code","colab":{}},"cell_type":"code","source":["learner = md.get_model(opt_fn, em_sz, nh, nl,\n","               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n","learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n","learner.clip=0.3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2u7yq_Ui_0R_","colab_type":"text"},"cell_type":"markdown","source":["As you can see below, I gradually tuned the language model in a few stages. I possibly could have trained it further (it wasn't yet overfitting), but I didn't have time to experiment more. Maybe you can see if you can train it to a better accuracy! (I used `lr_find` to find a good learning rate, but didn't save the output in this notebook. Feel free to try running it yourself now.)"]},{"metadata":{"id":"5_lH-6SS_0SA","colab_type":"code","outputId":"23594d52-baa0-4761-905f-e4e1a212249d","colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33b180b9efa543579efc82f95e6d27bb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=15, style=ProgressStyle(description_width='initia…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":[" 93%|█████████▎| 4242/4583 [20:44<01:32,  3.69it/s, loss=4.9]"],"name":"stdout"}]},{"metadata":{"id":"SWg-bzp5_0SE","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.save_encoder('adam1_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LivnCUSZ_0SF","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.load_encoder('adam1_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4HUqyqJL_0SH","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.load_cycle('adam3_10',2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q0BbbGAW_0SI","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ug5AV3NZ_0SL","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.save_encoder('adam3_10_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3eBmg4Et_0SN","colab_type":"text"},"cell_type":"markdown","source":["In the sentiment analysis section, we'll just need half of the language model - the *encoder*, so we save that part."]},{"metadata":{"id":"LcyGYGuT_0SN","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.save_encoder('adam3_20_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KFxZDfDG_0SO","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.load_encoder('adam3_20_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lc_X1kAw_0SQ","colab_type":"text"},"cell_type":"markdown","source":["Language modeling accuracy is generally measured using the metric *perplexity*, which is simply `exp()` of the loss function we used."]},{"metadata":{"id":"NFSF61w6_0SQ","colab_type":"code","colab":{}},"cell_type":"code","source":["math.exp(4.165)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"27fI8OOU_0ST","colab_type":"code","colab":{}},"cell_type":"code","source":["pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PeZ23sbT_0SU","colab_type":"text"},"cell_type":"markdown","source":["### Test"]},{"metadata":{"id":"N00oADMO_0SU","colab_type":"text"},"cell_type":"markdown","source":["We can play around with our language model a bit to check it seems to be working OK. First, let's create a short bit of text to 'prime' a set of predictions. We'll use our torchtext field to numericalize it so we can feed it to our language model."]},{"metadata":{"id":"iHc0VjDu_0SV","colab_type":"code","colab":{}},"cell_type":"code","source":["m=learner.model\n","ss=\"\"\". So, it wasn't quite was I was expecting, but I really liked it anyway! The best\"\"\"\n","s = [spacy_tok(ss)]\n","t=TEXT.numericalize(s)\n","' '.join(s[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FbuwEMtz_0SX","colab_type":"text"},"cell_type":"markdown","source":["We haven't yet added methods to make it easy to test a language model, so we'll need to manually go through the steps."]},{"metadata":{"id":"MzweOAKT_0SX","colab_type":"code","colab":{}},"cell_type":"code","source":["# Set batch size to 1\n","m[0].bs=1\n","# Turn off dropout\n","m.eval()\n","# Reset hidden state\n","m.reset()\n","# Get predictions from model\n","res,*_ = m(t)\n","# Put the batch size back to what it was\n","m[0].bs=bs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"56qQRL5x_0SZ","colab_type":"text"},"cell_type":"markdown","source":["Let's see what the top 10 predictions were for the next word after our short text:"]},{"metadata":{"id":"eRvNYz5b_0Sa","colab_type":"code","colab":{}},"cell_type":"code","source":["nexts = torch.topk(res[-1], 10)[1]\n","[TEXT.vocab.itos[o] for o in to_np(nexts)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c8RiM9qK_0Sb","colab_type":"text"},"cell_type":"markdown","source":["...and let's see if our model can generate a bit more text all by itself!"]},{"metadata":{"id":"uTZ9Xodv_0Sc","colab_type":"code","colab":{}},"cell_type":"code","source":["print(ss,\"\\n\")\n","for i in range(50):\n","    n=res[-1].topk(2)[1]\n","    n = n[1] if n.data[0]==0 else n[0]\n","    print(TEXT.vocab.itos[n.data[0]], end=' ')\n","    res,*_ = m(n[0].unsqueeze(0))\n","print('...')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xp444gb8_0Se","colab_type":"text"},"cell_type":"markdown","source":["### Sentiment"]},{"metadata":{"id":"k1qZ4etv_0Sf","colab_type":"text"},"cell_type":"markdown","source":["We'll need to the saved vocab from the language model, since we need to ensure the same words map to the same IDs."]},{"metadata":{"id":"fno1VvPk_0Sf","colab_type":"code","colab":{}},"cell_type":"code","source":["TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nKLyKftn_0Si","colab_type":"text"},"cell_type":"markdown","source":["`sequential=False` tells torchtext that a text field should be tokenized (in this case, we just want to store the 'positive' or 'negative' single label).\n","\n","`splits` is a torchtext method that creates train, test, and validation sets. The IMDB dataset is built into torchtext, so we can take advantage of that. Take a look at `lang_model-arxiv.ipynb` to see how to define your own fastai/torchtext datasets."]},{"metadata":{"id":"owiYHT0O_0Si","colab_type":"code","colab":{}},"cell_type":"code","source":["IMDB_LABEL = data.Field(sequential=False)\n","splits = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aUQ_oFuI_0Sk","colab_type":"code","colab":{}},"cell_type":"code","source":["t = splits[0].examples[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uQk6vzF6_0So","colab_type":"code","colab":{}},"cell_type":"code","source":["t.label, ' '.join(t.text[:16])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"baEZ8-Cc_0Sq","colab_type":"text"},"cell_type":"markdown","source":["fastai can create a ModelData object directly from torchtext splits."]},{"metadata":{"id":"gAQhqqIF_0Sr","colab_type":"code","colab":{}},"cell_type":"code","source":["md2 = TextData.from_splits(PATH, splits, bs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vL31eYAv_0Sv","colab_type":"code","colab":{}},"cell_type":"code","source":["m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, \n","           dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3)\n","m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n","m3.load_encoder(f'adam3_20_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xlzbxi0G_0S1","colab_type":"text"},"cell_type":"markdown","source":["Because we're fine-tuning a pretrained model, we'll use differential learning rates, and also increase the max gradient for clipping, to allow the SGDR to work better."]},{"metadata":{"id":"WX3xmcqc_0S2","colab_type":"code","colab":{}},"cell_type":"code","source":["m3.clip=25.\n","lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sV9Ngjnb_0S4","colab_type":"code","colab":{}},"cell_type":"code","source":["m3.freeze_to(-1)\n","m3.fit(lrs/2, 1, metrics=[accuracy])\n","m3.unfreeze()\n","m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LYqVnCZ__0S7","colab_type":"code","colab":{}},"cell_type":"code","source":["m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KaW7OYW8_0TA","colab_type":"code","colab":{}},"cell_type":"code","source":["m3.load_cycle('imdb2', 4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yeoD2UYt_0TJ","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy_np(*m3.predict_with_targs())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oU7zO9iA_0TL","colab_type":"text"},"cell_type":"markdown","source":["A recent paper from Bradbury et al, [Learned in translation: contextualized word vectors](https://einstein.ai/research/learned-in-translation-contextualized-word-vectors), has a handy summary of the latest academic research in solving this IMDB sentiment analysis problem. Many of the latest algorithms shown are tuned for this specific problem.\n","\n","![image.png](attachment:image.png)\n","\n","As you see, we just got a new state of the art result in sentiment analysis, decreasing the error from 5.9% to 5.5%! You should be able to get similarly world-class results on other NLP classification problems using the same basic steps.\n","\n","There are many opportunities to further improve this, although we won't be able to get to them until part 2 of this course..."]},{"metadata":{"id":"_Zi-z4nU_0TL","colab_type":"text"},"cell_type":"markdown","source":["### End"]},{"metadata":{"id":"NV_hjF1M_0TM","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}